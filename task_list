 * LARGE_BLOB/LARGE_TEXT
   * write iterator, adding bytes first to the prefix, and then if it has a worm pointer type atached to it then append to the worm
    * text_blob_write_iterator
      {
          void* tupl;
          tuple_def* tpl_d;
          positional_accessor inline_data_accessor;

          uint32_t bytes_to_be_written_to_prefix = min(get_max_size_increment_allowed_for_element_in_tuple(tpl_d, inline_data_accessor + [0], tupl), input value passed);
          uint32_t bytes_written_to_prefix;
          // no more appending to prefix if the above 2 attributes become equal

          int is_short; // for short_text and short_blob, making below attributes needless

          worm_append_iterator* wai; // this will be set only if a worm was created and the inline prefix is full, invalid if is short is set
      }
    * functions
      * get_new_text_blob_write_iterator(void* tupl, tuple_def* tpl_d, positional_accessor inline_data_accessor)
      * delete_text_blob_write_iterator
      * write_text_blob_write_iterator
   * read iterator, to first read from the prefix first and then from worm pointer type attached to it, if the worm pointer exists
    * text_blob_read_iterator
      {
          user_value prefix;
          uint32_t bytes_read_from_prefix;
          // no more reading from the prefix if the bytes_read_from_prefix == prefix.string_or_blob_size

          int is_short; // for short_text and short_blob, making below attributes needless

          uint64_t extension_head_page_id;
          worm_read_iterator* wri; // this will be set only if a worm was fully read from the prefix, it is not_short and the next read is to be performed from the worm
      }
    * functions
      * get_new_text_blob_read_iterator(void* tupl, tuple_def* tpl_d, positional_accessor inline_data_accessor)
      * delete_text_blob_read_iterator
      * read_text_blob_read_iterator
      * clone_text_blob_read_iterator // shallow copy the object, if wri is present and valid, then clone it
   * compare function using the read iterator on both the inputs, read from which ever buffer whenever it gets exhausted
   * hashing function using the read iterator
   * generate prefix type from the large types, with specified prefix length, using the read iterator on one and write on the smaller larger one

 * LARGE_NUMERIC
   * generate short_numeric data type, consisting of sign bits 2, fixed sized signed integer, and variable sized blob type to store significant digits
   * generate their large_numeric counterpart, that have uint64_t as page_id of the worm following the short_numeric type, which contains the rest of the digits
   * compare function for large types
   * sign bits and the exponent are stored inline even in large types
   * you might think a large_numeric can not be compare with prefixes
   * for instance 645780623465123456.6876458076456345655946595
   * sign_bits - 2, exponent - 16 bit signed integer, mantissa variable number of decimal digits
   * sign bits 00 = -infinity, 01 - negative, 10 - positive, 11 - +infinity
    so this becomes
    sign bits = 0b10, exponent 17, mantissa = 6.457806234651234566876458076456345655946595
    now you can easily truncate byte, giving us approximation, to upto some number of significant digits
    sign bits = 0b10, exponent 17, mantissa = 6.45780623
    comparison goes like first compare sign bits in binary unsigned, then exponent in signed, and then mantissa
    remaining mantissa bits can be found in the worm as 4651234566876458076456345655946595
   * the exponent is kept at 16 bit signed integer, as it is expected that no human measurable calculations from planck's constant to distance of farthest observable object in universe in centimeters will ever need anythingmore than this, (yet we will have local variables hold this number in int64_t to avoid overflow errors)
   * storing mantissa is a big problem, we need to store digit groups as unsigned integers
   * an X byte big integer can store Y digits, then Y = log(256 ^ X) / log(10) = X * 8 * log(2) / log(10)
   * the table goes as follows
    X |    Y   ->  ceil(Y) |  Y/X
    1 |  2.408 ->  2       |  2
    2 |  4.816 ->  4       |  2
    3 |  7.224 ->  7       |  2.333
    4 |  9.632 ->  9       |  2.25
    5 | 12.041 -> 12       |  2.4
    6 | 14.449 -> 14       |  2.333
    7 | 16.857 -> 16       |  2.285
    8 | 19.265 -> 19       |  2.375
    * if you analyze carefully the most value of digits per bytes is the most efficient way to go, which is using 5 byte unsinged integer to store a group of 12 decimal digits
    * in local variables we will store the 5 byte integer in uint64_t while we will serialize deserialize 5 byte integer using SerializableInteger library's functions
    * first task is to build a function to set one of the 12 digits in a uint64_t

* LARGE_JSONB
  * get -> takes var array of integers and string and makes you point to the beginning of the data in json struct
  * build a one direction readabale json serialization type, where we only need to go forward until we reach the desired type
  * create_worm
  * uses JSONparser's node as input object